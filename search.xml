<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大规模建筑内景的渲染：Interior-Mapping</title>
      <link href="/2019/09/14/implementation-of-interior-mapping/"/>
      <url>/2019/09/14/implementation-of-interior-mapping/</url>
      
        <content type="html"><![CDATA[ <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/1.0_cover.png" alt title="最终的效果图"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Interior Mapping主要用于在物体表面利用透视错局模拟出格子结构的内部内容。虽然从外部观察起来，物体内部的构造是“真实的”，但一切渲染都没有依靠更多的模型或真实顶点。这在渲染大量模式化的内部结构中会有很大作用。<br>例如渲染高楼林立的都市时，如果要渲染每个单独的房间就会有大量的性能开销，完全的使用单一的贴图又会有些单调，这个时候就适合用Interior Mapping来伪造出建筑的外观而不让性能下降太多。<br>它的几个优点：<br>1.房间的数量不会影响framerate或内存开销<br>2.并不需要太多的额外资源就可以表现大的场景<br>3.实现上并不要求高级的shader model特性<br><a id="more"></a></p><h1 id="基本思路的实现"><a href="#基本思路的实现" class="headerlink" title="基本思路的实现"></a>基本思路的实现</h1><p>思路可以参考Joost van Dongen的论文<a href="https://pdfs.semanticscholar.org/8622/48de620efe27705af3702ab2a2c0d4ec76ec.pdf" target="_blank" rel="noopener">Interior Mapping - A new technique for rendering realistic buildings</a><br>按照Joost论文中的思路实现即可，本文不再详述。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/1.1_basic_idea.png" alt title="论文中的图示和下面代码的对应关系"><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reciprocal of each floor's height d</span></span><br><span class="line">float3 invert_d = _Tiling / _BoundarySize;</span><br><span class="line"></span><br><span class="line">float3 cameraPos_obj = mul(unity_WorldToObject, float4(_WorldSpaceCameraPos, <span class="number">1.0</span>)).xyz;</span><br><span class="line">float3 cameraToPixelOffset_obj = i.pos_obj - cameraPos_obj;</span><br><span class="line"></span><br><span class="line">float3 cameraStepDir_obj = step(float3(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), cameraToPixelOffset_obj);</span><br><span class="line">float3 floorId = <span class="built_in">floor</span>(i.pos_obj * <span class="number">0.999</span> * invert_d)  + cameraStepDir_obj;</span><br><span class="line">float3 floorPos_obj = floorId / invert_d;</span><br><span class="line">float3 cameraToFloorIntersectionOffset_obj = floorPos_obj - cameraPos_obj;</span><br><span class="line"><span class="comment">// This is the ratio between the actual distance before reaching destination and the distance of pixel and camera in object space</span></span><br><span class="line">float3 ratio = cameraToFloorIntersectionOffset_obj / cameraToPixelOffset_obj;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Here the intersection coordinate has been normalized through dividing by d</span></span><br><span class="line">float2 intersectionXY_obj = (cameraPos_obj + ratio.z * cameraToPixelOffset_obj).xy * invert_d;</span><br><span class="line">float2 intersectionXZ_obj = (cameraPos_obj + ratio.y * cameraToPixelOffset_obj).xz * invert_d;</span><br><span class="line">float2 intersectionZY_obj = (cameraPos_obj + ratio.x * cameraToPixelOffset_obj).zy * invert_d;</span><br><span class="line"></span><br><span class="line">float4 ceilingCol = tex2D(_CeilingTexture, intersectionXZ_obj);</span><br><span class="line">float4 floorCol = tex2D(_FloorTexture, intersectionXZ_obj);</span><br><span class="line">float4 horizonCol = lerp(floorCol, ceilingCol, step(<span class="number">0</span>, cameraToPixelOffset_obj.y));</span><br><span class="line"></span><br><span class="line">float4 wallXYCol = tex2D(_WallXYTexture, intersectionXY_obj);</span><br><span class="line">float4 wallZYCol = tex2D(_WallZYTexture, intersectionZY_obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Check which face is closer to camera and pick it as the final texuture for specific pixel</span></span><br><span class="line"><span class="keyword">float</span> xLessThanZ = step(ratio.x, ratio.z);</span><br><span class="line">float4 verticalCol = lerp(wallXYCol, wallZYCol, xLessThanZ);</span><br><span class="line"><span class="keyword">float</span> ratioMin_x_z = lerp(ratio.z, ratio.x, xLessThanZ);</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> x_zLessThanY = step(ratioMin_x_z, ratio.y);</span><br><span class="line">float4 innerCol = lerp(horizonCol, verticalCol, x_zLessThanY);</span><br></pre></td></tr></table></figure></p><p>这段按照论文思路实现的代码有几点缺陷：<br>1.墙面还没有厚度，虽然可以手动加一个外墙的mask但是，会从窗外看到一个不对齐的地面，同时也无法只通过一个厚度参数自动调整。<br>2.还没有加上sprite层（家具层）丰富室内内容<br>3.在拐角处会看到本不应该看到的墙面</p><p>本文的一些定义：<br>Block: 每个单独的房间称为一个Block<br>内腔：每个Block中不被墙体填充的部分<br>d: 代表一个Block某个方向上的长度<br>_BoundarySize: 指一个Cube Mesh的某条棱的长度<br>_Tiling: 指在某个坐标轴方向上，Mesh被切分的次数<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/0.2_conventions.png" alt title="本文中出现的一些变量图示"></p><h1 id="可配置厚度墙面的实现"><a href="#可配置厚度墙面的实现" class="headerlink" title="可配置厚度墙面的实现"></a>可配置厚度墙面的实现</h1><p>如果没有显示出墙的厚度，会有看上去错位的地板<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.0_without_wall_thickness.png" alt title="图片来源[6]"></p><p>我们要做的是将原本的墙面朝着某个方向推出一定距离，模拟出墙的厚度。首先计算出摄像机相关变量，方便后面确定偏移量<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">float3 cameraPos_obj = mul(unity_WorldToObject, float4(_WorldSpaceCameraPos, <span class="number">1.0</span>)).xyz;</span><br><span class="line">float3 cameraToPixelOffset_obj = i.pos_obj - cameraPos_obj;</span><br><span class="line">float3 cameraStepDir_obj = step(float3(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), cameraToPixelOffset_obj); <span class="comment">// return 1 if it's positive, otherwise 0</span></span><br></pre></td></tr></table></figure></p><p>原始的墙面界面位置为<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">floor</span>(i.positionCopy * <span class="number">0.999</span> / _d)  + stepDir</span><br></pre></td></tr></table></figure></p><p>为了模拟墙面向内intrude，需要减去一个墙面的偏移量<code>(stepDir - float3(0.5, 0.5, 0.5)) * _WallThickness</code>之后再计算相似三角形边的比<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">float3 ratio = ((<span class="built_in">floor</span>(i.positionCopy * <span class="number">0.999</span> / _d)  + stepDir - (stepDir - float3(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)) * _WallThickness) * _d  - cameraPos_obj) / direction;</span><br></pre></td></tr></table></figure></p><p>不过目前为止只是加上了一个墙面的偏移量，结果如下图一样<div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.1_wrong_wall.png" alt title="本该被挡住的地方因为显示了出来，会呈现错误的透视感觉"></div><br>造成视觉上的怪异的原因是对于墙面部分，我们俯视它时依然会“穿过”水平墙面看到内部的垂直墙面，而这些uv之外的部分实际不应该被看到。</p><h2 id="判断像素是否在墙内"><a href="#判断像素是否在墙内" class="headerlink" title="判断像素是否在墙内"></a>判断像素是否在墙内</h2><p>找到问题之后，我们就需要找到每个block中，那些像素对应的是墙面上的位置。<br>首先利用floor可以找到每个block的中心位置。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">float3 floorId = <span class="built_in">floor</span>(i.vertexPos  * <span class="number">0.999</span> / _d);</span><br><span class="line">float3 centerPos_obj = (floorId + <span class="number">0.5</span>) * _d;</span><br><span class="line">float3 offsetFromCenter_obj = <span class="built_in">abs</span>(i.vertexPos - centerPos_obj) / _d;</span><br></pre></td></tr></table></figure></p><p>这里的centerPos_obj在object space中，offsetFromCenter_obj表示的是归一化后的偏移，即范围为[0,1]，代表偏移量占每个block的比例.</p><p>至于如何判定一个点是否处在墙面里，一种方法是：<br>检测在某个平面上最远的坐标分量是否超出了内cube的半径，以xy平面为例<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> max_xy = max(offsetFromCenter_obj.x, offsetFromCenter_obj.y);</span><br><span class="line"><span class="keyword">float</span> mask = step((<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>, max_xy );</span><br></pre></td></tr></table></figure></p><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.2_xy_bool_mask.png" alt title></div><br>同样的手法可以用到其他2个平面上，合起来就是<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> max_xy = max(offsetFromCenter_obj.x, offsetFromCenter_obj.y);</span><br><span class="line"><span class="keyword">float</span> mask = step((<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>, max_xy );</span><br><span class="line"><span class="keyword">float</span> max_xz = max(offsetFromCenter_obj.x, offsetFromCenter_obj.z);</span><br><span class="line">mask *= step((<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>, max_xz );</span><br><span class="line"><span class="keyword">float</span> max_yz = max(offsetFromCenter_obj.y, offsetFromCenter_obj.z);</span><br><span class="line">mask *= step((<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>, max_yz );</span><br></pre></td></tr></table></figure><br><br><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.3_all_bool_mask.png" alt title></div><br>这里mask相当于是一个AND布尔运算操作，只有在各个平面的投影中都属于“内部”中的像素才可以被称为真正的内腔，也就是会绘制墙面上的像素，这些像素的mask值为0.<br>这个过程可以想象成在三个正交的方向上对一个几何体做投影挖去不属于墙面的部分，最终剩下的白色部分就是我们所需要的墙面<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">float4 outerCol = tex2D(_outerWallTexture, i.uv);</span><br><span class="line">float4 finalCol = lerp(innerCol, outerCol, mask);</span><br></pre></td></tr></table></figure><br><br>最终的效果如<br><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.4_correct_wall.png" alt title></div><h2 id="使用AND还是OR"><a href="#使用AND还是OR" class="headerlink" title="使用AND还是OR"></a>使用AND还是OR</h2><p>我们在进行投影布尔运算的时候，实际上会将内部结构中的墙体一并减去。但由于我们实际上只需要最外层的mask值，所以对于立方体来说，这样的操作不会有什么问题。但是对于球体等不规则物体来说，由于我们会看到内部的结构，所以被错误减去的墙体将会出现视觉问题。</p><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.5_wrong_sphere.png" alt title></div><br>如果将其mask输出，就能更容易的发现问题<br><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.6_wrong_sphere_mask.png" alt title></div><br>本来是内部的墙面出现在了球面上，因为被不正确的减去了，所以本来应该有墙的地方就缺失了一块.<br>可能从正交视图的某个平面方向上看更容易理解<br><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.7_wrong_sphere_mask_top.png" alt title="这里把部分xz平面上投影形成的mask用红色的框标注出来了"></div><p>修正这个错误的思路就是从之前的AND运算变成OR运算，只要在任意方向上超出内腔的范围，就认定为进入墙体。这从直观理解上也更容易理解。<br>值得注意的是，如果用OR运算，我们只需要判断两个方向即可。这是因为第三个方向的信息变得冗余了。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> max_xy = max(offsetFromCenter_obj.x, offsetFromCenter_obj.y);</span><br><span class="line"><span class="keyword">float</span> mask = step(max_xy, (<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>);</span><br><span class="line"><span class="keyword">float</span> max_xz = max(offsetFromCenter_obj.x, offsetFromCenter_obj.z);</span><br><span class="line">mask *= step(max_xz, (<span class="number">1</span> - _WallThickness) * <span class="number">0.5</span>);</span><br><span class="line"><span class="comment">// float max_yz = max(offsetFromCenter_obj.y, offsetFromCenter_obj.z);</span></span><br><span class="line"><span class="comment">// mask *= step(max_yz, (1 - _WallThickness) * 0.5);</span></span><br><span class="line">mask = <span class="number">1</span> - mask;</span><br></pre></td></tr></table></figure></p><div style="width: 60%; margin: auto"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.8_correct_sphere_mask.png" alt title="可以看到这次终于正确了。"></div><p>不过这也会带来一个潜在的问题，<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/2.9_potential_visual_bug.png" alt title><br>当tiling达到某个值的时候，墙面会恰好在立方体的边缘，这个时候，整个表面就被墙遮住了。我曾经想在shader中判断并自动做偏移（只有tiling为偶数的时候才会出现）不过后来还是决定暴露这个参数，让使用者自己决定合适的tiling。有趣的是，虽然这个问题造成了不少困扰，但是之前AND的操作方法是不会遇到这个问题的;)<br>解决方法：判断墙的边界是否超出mesh的边界，详见<a href="#culling_Wall">正确显示边角的遮挡</a></p><h1 id="制作Sprite-Layer"><a href="#制作Sprite-Layer" class="headerlink" title="制作Sprite Layer"></a>制作Sprite Layer</h1><h2 id="朴素的截面偏移层"><a href="#朴素的截面偏移层" class="headerlink" title="朴素的截面偏移层"></a>朴素的截面偏移层</h2><p>原论文中提到了制作在空房间内放置道具或者人物的思路，其实实现起来也很简单，和墙面一样是对观察深度进行一个偏移后计算交点。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Intrude XY wall along Z axis by 0.4 block length</span></span><br><span class="line">float3 spriteOffset_block = float3(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0.4</span>);</span><br><span class="line">float3 cameraStepDir_obj = step(float3(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), cameraToPixelOffset_obj);</span><br><span class="line">float3 floorId = <span class="built_in">floor</span>(i.pos_obj * <span class="number">0.999</span> * invert_d) + cameraStepDir_obj;</span><br><span class="line">float3 spriteLayerRatio = ((floorId - spriteOffset_block) / invert_d  - cameraPos_obj) / cameraToPixelOffset_obj;</span><br><span class="line">float2 S_intersectionXY = (cameraPos_obj + spriteLayerRatio.z * cameraToPixelOffset_obj).xy  * invert_d;</span><br><span class="line">float2 S_intersectionZY = (cameraPos_obj + spriteLayerRatio.x * cameraToPixelOffset_obj).zy  * invert_d;</span><br><span class="line">float4 S_XYCol = tex2D(_SpriteTex, S_intersectionXY);</span><br><span class="line">float4 S_ZYCol = tex2D(_SpriteTex, S_intersectionZY);</span><br><span class="line"><span class="keyword">float</span> S_xLessThanZ = step(spriteLayerRatio.x, spriteLayerRatio.z);</span><br><span class="line">float4 S_verticalCol = lerp(S_XYCol, S_ZYCol, S_xLessThanZ);</span><br><span class="line"><span class="keyword">float</span> S_ratioMin_x_z = lerp(spriteLayerRatio.z, spriteLayerRatio.x, S_x_less_z);</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> ratioMin_xyz = lerp(ratio.y, ratioMin_x_z, xzLessThanY);</span><br><span class="line"><span class="comment">// To see if it's closer than the origin wall</span></span><br><span class="line"><span class="keyword">float</span> S_isLess = step(S_ratioMin_x_z, ratioMin_xyz);</span><br><span class="line"><span class="comment">// If sprite layer is closer and the alpha is greater than 0, replace old pixel of wall by sprite layer</span></span><br><span class="line">innerCol = lerp(innerCol, S_verticalCol, S_isLess * S_verticalCol.a);</span><br></pre></td></tr></table></figure><p>不过这种方法也会有明显的问题，就是只能从一个方向上观察室内的人物或物体，从侧面和背面都能很容易的注意到是面片这一事实。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.1_view_different_angle.gif" alt title="从各个角度观察"></p><h3 id="支持多方向适配"><a href="#支持多方向适配" class="headerlink" title="支持多方向适配"></a>支持多方向适配</h3><p>我们可以手工指定其他面，从而正确的显示<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// If the tile's unilateral count(from the center to this tile) reach half of total tiling, means it's the side tile</span></span><br><span class="line"><span class="keyword">float</span> isSideOrFront = <span class="built_in">abs</span>(<span class="built_in">ceil</span>(i.pos_obj * <span class="number">0.999</span> * invert_d).x) / (_Tiling.x/<span class="number">2</span>) &gt; <span class="number">0.999</span> ? <span class="number">1</span>:<span class="number">0</span>;</span><br><span class="line"><span class="comment">// Pick the color from side texture</span></span><br><span class="line">float4 S_verticalCol = lerp(S_XYCol, S_ZYCol, isSideOrFront);</span><br><span class="line"><span class="comment">// Here we use the ratio of the face we decided to use in above step to overwrite the actual minimal one "S_x_less_z"</span></span><br><span class="line"><span class="keyword">float</span> S_ratioMin_x_z = lerp(spriteRatio.z, spriteRatio.x, isSideOrFront);</span><br><span class="line">float2 halfTileCount = lerp(S_intersectionXY_ori, S_intersectionZY_ori, isSideOrFront) / _Tiling.x * <span class="number">2</span>;</span><br><span class="line"><span class="comment">// Because our sprite will keep repeating, even in the place we should not see it. Luckily, we can use the time it repeats to see if it's out of the valid display region.</span></span><br><span class="line"><span class="keyword">bool</span> validSpriteRegion = <span class="number">1</span></span><br><span class="line">    * step(<span class="built_in">abs</span>(halfTileCount.y), <span class="number">1</span>) </span><br><span class="line">    * step(<span class="built_in">abs</span>(halfTileCount.x), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> ratioMin_xyz = lerp(ratio.y, S_ratioMin_x_z, xzLessThanY);</span><br><span class="line"><span class="keyword">float</span> S_isLess = step(S_ratioMin_x_z, ratioMin_xyz) * validSpriteRegion;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p><img src="https://public-covers-1259535704.cos.ap-guangzhou.myqcloud.com/3.2_dirs_sprites.png" alt title="可以看到效果还是不理想"></p><h2 id="类Billboard-Sprite-Layer"><a href="#类Billboard-Sprite-Layer" class="headerlink" title="类Billboard Sprite Layer"></a>类Billboard Sprite Layer</h2><p>上面的方法除了在适应不同视角上处理的很生硬，代码中也有很多硬编码的重复代码。一个比较优雅的改进方法是使用一种技术可以自动的适应不同的观察角度。所以我在这里的方法是利用billboard思想制作一个可以自动跟随摄像机的sprite</p><h3 id="传统Billboard"><a href="#传统Billboard" class="headerlink" title="传统Billboard"></a>传统Billboard</h3><p>传统的billboard思路是变换顶点坐标：将Object Space下的坐标直接赋值给View Space，即可以获得稳定不变的view位置。几乎所有的操作也都是在vertex shader中完成的<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">float4 billboardPos = mul(UNITY_MATRIX_P,</span><br><span class="line">    mul(UNITY_MATRIX_MV, float4(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>)) <span class="comment">// 依旧要记录物体中心的位置，否则物体始终在屏幕中央</span></span><br><span class="line">    + float4(v.vertex.x, v.vertex.y, <span class="number">0.0</span>, <span class="number">0.0</span>));</span><br></pre></td></tr></table></figure></p><p>但是显然这种方法无法运用到我们这个例子上，因为其他部分的显示要依附于原始的顶点位置，另外如果我们有多个block，那么对于每个block，我们无法再分出更多的顶点来完成变换（不会考虑Gemotry Shader)</p><h3 id="物体空间内的Billboard"><a href="#物体空间内的Billboard" class="headerlink" title="物体空间内的Billboard"></a>物体空间内的Billboard</h3><p>我们的思路是在保持顶点位置不变的情况下，算出每个像素对应的立方体表面的一个点的投影坐标</p><h3 id="基础知识准备"><a href="#基础知识准备" class="headerlink" title="基础知识准备"></a>基础知识准备</h3><p>在一般的渲染管线中。最先传入的顶点数据都是模型空间下的(Object space/Model space)，经过Model Matrix，View Matrix和Projection Matrix以及Viewport Transform之后成为屏幕上的坐标。在不同的文章中，Projection Matrix的定义会有差别，也是比较容易引起误导的地方。我们这里将Projection Matrix定义为将物体从View Space(Eye Space)转换到NDC Space的矩阵。<br>这样一个完整的Projection Matrix包括两个部分，<br>第一部分是“线性”的缩放，将原本的View Frustum到近裁面为$[-Z_{near}, Z_{near}]$,但深度只有1的“相似” View Frustum。在这个Clip Space中，任意一点的属性都可以由顶点属性基于x,y,z中的任意一个进行插值而得到。<br>$$<br>\left[<br>\begin{array}{cccc}<br> x_{clip}\\<br> y_{clip}\\<br> z_{clip}\\<br> 1<br>\end{array}<br>\right ]<br>=<br>\left[<br>\begin{array}{cccc}<br> \frac{ {2n} }{ {r-l} } &amp; 0 &amp; \frac{ {r+l} }{ {r-l} }  &amp; 0 \\<br> 0 &amp; \frac{ {2n} }{ {t-b} } &amp; \frac{ {t+b} }{ {t-b} }  &amp; 0 \\<br> 0 &amp; 0                      &amp; \frac{ {-(f+n)} }{ {f-n} } &amp; \frac{ {-2fn} }{ {f-n} } \\<br> 0 &amp; 0 &amp; -1 &amp; 0 \\<br>\end{array}<br>\right ]<br>\left[<br>\begin{array}{cccc}<br> x_{view}\\<br> y_{view}\\<br> z_{view}\\<br> 1<br>\end{array}<br>\right ]<br>$$<br>这一步发生在vert shader中<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/camera-space.png" alt title="图中的是Camera Space，但Clip Space也是类似的形状,。图片来自[1]"><br>第二部分中，我们要将Clip Space通过Perspective Divide转换到NDC(Normalized Device Coordinates) Space。NDC空间是按近大远小压缩的。<br>NDC还有另外一个特点：该空间中如果要计算任意一点的属性值，直接根据Barycentric Interpolation来插值的话会因为深度的非均匀压缩而产生扭曲<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/perspective_correct_texture_mapping.jpg" alt title="中间为没有透视矫正的场景，图片来自wikipedia"><br>解决方法是应用Perspective Correction进行矫正，若已知顶点P1, P2的属性，则线段上任意一点P的值为<br>$$p=z[\frac{p_1}{z_1}(1-t) + \frac{p_2}{z_2}t]$$<br>直观的理解是先将属性按照深度压缩，然后就可以线性的插值了。详细的推导过程见<a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/visibility-problem-depth-buffer-depth-interpolation" target="_blank" rel="noopener">scratchapixel</a></p><p>Perspective Divide的矩阵表示<br>$$<br>\left[<br>\begin{array}{cccc}<br> x_{ndc}\\<br> y_{ndc}\\<br> z_{ndc}<br>\end{array}<br>\right ]<br>=<br>\left[<br>\begin{array}{cccc}<br>    \frac{1}{-z_{view}}\\<br>    \frac{1}{-z_{view}}\\<br>    \frac{1}{-z_{view}}<br>\end{array}<br>\right ]^{\mathsf{T}}<br>\left[<br>\begin{array}{cccc}<br> x_{clip}\\<br> y_{clip}\\<br> z_{clip}<br>\end{array}<br>\right ]<br>$$<br>这一步在frag shader之前GPU自动进行<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/ndc.png" alt title="注意此时z轴的方向会变成相反的。 图片来自[1]"><br>如果将clip space中的属性应用到屏幕上，会因为顶点的位置是压缩过的而产生变形。</p><p>这样有了两个部分后，一个完整的Projection Matrix就可以表示为<br>$$<br>[M_{Proj}] =<br>\left[<br>\begin{array}{cccc}<br>    \frac{1}{-z_{view}}\\<br>    \frac{1}{-z_{view}}\\<br>    \frac{1}{-z_{view}}\\<br>    N/A<br>\end{array}<br>\right ]^{\mathsf{T}}<br>\left[<br>\begin{array}{cccc}<br> \frac{ {2n} }{ {r-l} } &amp; 0 &amp; \frac{ {r+l} }{ {r-l} }  &amp; 0 \\<br> 0 &amp; \frac{ {2n} }{ {t-b} } &amp; \frac{ {t+b} }{ {t-b} }  &amp; 0 \\<br> 0 &amp; 0                      &amp; \frac{ {-(f+n)} }{ {f-n} } &amp; \frac{ {-2fn} }{ {f-n} } \\<br> 0 &amp; 0 &amp; -1 &amp; 0<br>\end{array}<br>\right ]<br>$$</p><p>如果直接使用frag shader的输入中的position（通过SV_POSITION这个语义绑定(semantics binding) ）的xy分量来映射billboard的uv，我们可以得到一个屏幕空间下的映射，但是使用的其实就是类似于gl_FragCoord的屏幕坐标值，并不能适应我们后面的调整需要。另外补充一下，SV_POSITION虽然在vert shader还是Clip Space，但是在进入frag shader之前会进行Perspective Divide（在某些硬件下这部分的计算会交给frag shader，但是仍处于可编程部分之前，所以可以通俗的认为都发生在frag shader之前）从而SV_POSITION就进入了NDC也即就行了透视处理。<br>虽然Unity中DirectX和OpenGL或Vulkan对projection matrix的实现各不相同，但是他们的w分量都是$Z_{view}$(虽然Projection Matrix中$w = -Z_{view}$，但在前一步的View Matrix时已经取反了一次)，所以除以这个w分量就能将坐标压缩到NDC完成透视变换。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.3_unity_matrix.png" alt title><br>屏幕空间下的值只能使用NDC来插值，Clip Space下坐标和View Space成“线性”关系，和World Space成affine关系。</p><p>虽然frag shader中的输入SV_POSITION的z值已经是NDC下的，但是因为我们后面会要计算相对参考点的位置，而参考点又都没有进行透视处理，所以为了方便起见，我们统一在Clip Space进行处理，最后一步的时候再转换到NDC空间。<br>而要获得像素点在Clip Space下的坐标，我们有两种方法，<br>一种是对已经进入到ndc的坐标逆向乘以w分量从而得到Clip Space坐标。不过因为SV_POSITION本身并不属于NDC，所以我们不能通过这种方法获取除z以外的其他Clip Space中的坐标<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> depth = i.pos.z * i.pos.w;</span><br><span class="line"><span class="keyword">return</span> half4(depth, depth, depth, <span class="number">1</span>); <span class="comment">// Ouput depth visualization</span></span><br></pre></td></tr></table></figure></p><p>还有一种方法是现在vert shader中将Clip Space坐标保存成一个顶点属性，然后交由硬件插值后传入frag shader。这里的重要区别是，GPU不会对没有标记SV_POSITION的顶点属性进行Perspective Divide和ViewPort Transform，进行的是含透视矫正的插值。由于是含透视矫正的，所以对于插值的属性，它按在Clip Space的深度进行插值，也就是说它的坐标值准确的对应于Clip Space下该frag的坐标。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> depth = i.pos_clip.z;</span><br><span class="line"><span class="keyword">return</span> half4(depth, depth, depth, <span class="number">1</span>); <span class="comment">// Ouput depth visualization</span></span><br></pre></td></tr></table></figure></p><p>有意思的是，如果我们将Clip Space坐标除以了w之后就可以得到NDC空间下的屏幕坐标，可以用来当uv采样texture看看是不是如我们期望的一样。<br>核心代码如下<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">v2f</span> &#123;</span></span><br><span class="line">    float4 pos:     SV_POSITION;</span><br><span class="line">    float2 uv:      TEXCOORD0;</span><br><span class="line">    float4 pos_clip:TEXCOORD1;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">v2f <span class="title">vert</span><span class="params">(appdata_all v)</span> </span>&#123;</span><br><span class="line">    v2f o;</span><br><span class="line">    ...</span><br><span class="line">    o.pos = UnityObjectToClipPos(v.vertex);</span><br><span class="line">    o.pos_clip = UnityObjectToClipPos(v.vertex);</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line">half4 frag(v2f i) : COLOR &#123;</span><br><span class="line">    <span class="keyword">return</span> tex2D(_MainTex, frac(i.pos_clip.xy / i.pos_clip.w));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.4_rotate_first_billboard.gif" alt title><p>但是我们也会注意到一个问题，这是一个“真的”billboard，uv的中心也始终在屏幕（也就是摄像机）的中央，如果将摄像机进行平移，就会失去中心点。另外，随着摄像机的拉近和拉远也没有缩放效果。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.5_static_scale.gif" alt title="这与其说是billboard，倒不如说更像是个用几何体的轮廓制作的Mask"></p><h3 id="让Billboard跟随物体"><a href="#让Billboard跟随物体" class="headerlink" title="让Billboard跟随物体"></a>让Billboard跟随物体</h3><p>让我们先解决第一个问题，即让billboard跟着mesh走。方法是先计算出物体模型空间下的中心在Clip Space中的位置。由于已经有了当前frag的Clip Space坐标，我们就能算出当前frag距离物体中心的偏移量（注意此刻我们仍是在Clip Space中，后面还需转换到NDC中）。如果当前frag的Clip Space坐标和物体的中心一样，那么偏移量就为0，uv也对应的为0。在有更多偏移量的地方设置相应的uv，这样就实现了uv以<strong>物体中心</strong>而不是屏幕的中心为起点向周围发散。这会是一个很大的帮助，因为我们后面会要让每个billboard sprite紧紧附在对应的block上。</p><p>不过如果uv的起点是物体的中心，那贴图的左下角（UV为(0,0)）也会是会和中心对齐，这看上去就会很奇怪。我们要做的是指定GetNdcUv计算出的uv变为（0.5，0.5），即在计算出的uv加上（0.5，0.5）的偏移（等价于将texture向（-0.5，-0.5）的方向移动）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">float3 center_block = float3(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>);</span><br><span class="line">float3 offsetFromFloor_block = frac(<span class="number">1</span> + sign(i.pos_obj) * center_block);</span><br><span class="line"><span class="comment">// Origin we want the billboard to be in object space</span></span><br><span class="line">float3 offset_obj = sign(i.pos_obj) </span><br><span class="line">        * d * (offsetFromFloor_block + <span class="built_in">floor</span>(<span class="built_in">abs</span>(i.pos_obj * <span class="number">0.999</span>) / d));</span><br><span class="line">float4 uvOrigin_clip = UnityObjectToClipPos(float4(offset_obj.xyz, <span class="number">1.0</span>));</span><br><span class="line">float2 uv_ndc = pos_clip.xy / pos_clip.w;</span><br><span class="line">float2 uv_origin_ndc = uvOrigin_clip.xy / uvOrigin_clip.w;</span><br><span class="line"><span class="comment">// Caculate the uv offset from center in obj space</span></span><br><span class="line">uv_ndc.xy -= uv_origin_ndc;</span><br><span class="line">half4 screenTexture = tex2D(_MainTex, uv_ndc + center_block); <span class="comment">// The compensate offset here is actually incorrect since we only consider one direction.</span></span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.6_uv_correction.png" alt title><p>可以看到A1格已经到了左下角，没有完全对齐的原因是因为缩放比例还不正确。</p><p>我们也可以调节uvTiling让uv充满整个面（这个也可以自动算出，但是有时候美术的确可能需要手动调节sprite的大小）,如果想让sprite的anchor中心不再是物体的中心，也可以自己加offset细调，这会在<a href="#anchro_adjust">后面</a>提到。</p><h3 id="让Billboard带透视"><a href="#让Billboard带透视" class="headerlink" title="让Billboard带透视"></a>让Billboard带透视</h3><p>至于没有随着距离远近缩放的问题，可以通过<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv_ndc *= uvOrigin_clip.w;</span><br></pre></td></tr></table></figure></p><p>来解决，这是因为uv_ndc是在ndc下的offset，只有乘以了z才能变成Clip Space下的距离，实现uv“近小远大”，texture对应的近大远小。另外注意这里对于一个block，统一使用了中心点的w分量来做透视，这样可以保证整个面上所有的像素的深度都是一致的。虽然也可以通过与frag的w混合来实现一定程度的透视效果，但是这里就不展开了。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.7_perspective_correct.gif" alt title></p><h3 id="Tiled-Sprite-Layer"><a href="#Tiled-Sprite-Layer" class="headerlink" title="Tiled Sprite Layer"></a>Tiled Sprite Layer</h3><p>下一步就是实现 tilied sprite layer。<br>我们可以首先计算出每个block的边长<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> d = _BoundarySize/ _Tiling;</span><br></pre></td></tr></table></figure></p><p>同时将原始的object position也作为顶点属性传递到frag shader之后，可以计算出这属于第几个block，然后再加上半格的偏移就可以得到每个block中心的位置，要记得带上sign(i.originPos)，否则方向会是错的。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> d = _Length / _PosTiling;</span><br><span class="line"><span class="comment">// Origin we want the billboard to be in object space</span></span><br><span class="line">float3 offset_obj = sign(i.pos_obj) * d * (<span class="built_in">floor</span>(<span class="built_in">abs</span>(i.pos_obj*<span class="number">0.999</span>) / d) + float3(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>));</span><br></pre></td></tr></table></figure></p><p>再将新的offset传入后，就能看到tilied的sprite了<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.8_tiled_billboard.gif" alt title></p><p><span id="anchro_adjust"></span><br>目前物体的旋转中心还是在物体的中央（每个block的(0.5,0.5,0.5)处）。我们要调整的是offset，因为它是我们认为的参考点（anchor），所以还要做一步偏移操作。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Move the anchor to feet</span></span><br><span class="line">float3 center_block = float3(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>) + float3(<span class="number">0</span>, <span class="number">-0.3</span>, <span class="number">0</span>);</span><br><span class="line">float3 offsetFromFloor_block = frac(<span class="number">1</span> + sign(i.pos_obj) * center_block);</span><br><span class="line">float3 offset_obj = sign(i.pos_obj) </span><br><span class="line">    * d * (offsetFromFloor_block + <span class="built_in">floor</span>(<span class="built_in">abs</span>(i.pos_obj * <span class="number">0.999</span>) / d));</span><br><span class="line">float3 intrudeDir_obj = normalize(offset_obj - cameraPos_obj);</span><br><span class="line">float2 uv_ndc = GetNdcUv(clipPos, _UvTiling, offset_obj.xyz - _Offset.w * intrudeDir_obj));</span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.9_align_to_anchor.gif" alt title="我将anchor gizmos放到了billboard在的中心，可以看到无论如何旋转相机，billboard的uv都能和它保持相对静止"><p>为了验证我们的计算没有错误，我们可以比较虚拟billboard是否和真实的billboard有一样的现实。我在场景里放了一块传统的billboard，放置在和虚拟billboard同样的anchor点，然后旋转物体进行观察。结果可以看到两者完全重合在一起，证明了算法的准确性。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.10_compare_with_real_billboard.gif" alt title="你能发现藏在box中的真正的billboard吗"></p><p><img src="https://4.bp.blogspot.com/-fjxkTPOjjFY/W6YYGka4GeI/AAAAAAAAHZM/Z3cqOgvNANY76Ho3-_B81p1ZdmHa2j2DwCLcBGAs/s1600/Interior%2BMapping%2B-%2BFurniture.jpg" alt title=" 比较论文中的家具层的效果，可以看到会在某种程度上减小flat感"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/3.11_final_result.gif" alt title="最终效果"></p><p><span id="culling_Wall"></span></p><h1 id="正确显示边角的遮挡"><a href="#正确显示边角的遮挡" class="headerlink" title="正确显示边角的遮挡"></a>正确显示边角的遮挡</h1><p>最后一步是，如果墙面正好在房间的外部一些，即我们的mesh之外一点。虽然表面上采样的点属于内腔，从正面看是不会看到遮挡的墙，但是从另一个角度看，则会看到本该被剔除的墙面。一般发生在墙面的转角处。这一点甚至在PS4 Spider-Man中都没有处理<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/4.1_wrong_corner_spider_man.png" alt title="注意画面左侧的墙面，本应该是窗户但是却显示了墙壁"><br>完整视频参考：</p><iframe width="560" height="315" src="https://www.youtube.com/embed/YQVHtlVEirs?start=39" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p>不过要想解决这个问题并不复杂。我们只用计算出最外层的坐标阈值，超出部分一律不绘制即可。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Test if wall's z is out of box boundary</span></span><br><span class="line"><span class="keyword">float</span> zIsInsideBoundary = step(<span class="built_in">abs</span>(cameraPos_obj + ratio.z * cameraToPixelOffset_obj).z / (_BoundarySize * <span class="number">0.5</span>), <span class="number">0.999</span>);</span><br><span class="line"><span class="comment">// Test if wall's x is out of box boundary</span></span><br><span class="line"><span class="keyword">float</span> xIsInsideBoundary = step(<span class="built_in">abs</span>(cameraPos_obj + ratio.x * cameraToPixelOffset_obj).x / (_BoundarySize * <span class="number">0.5</span>), <span class="number">0.999</span>);</span><br><span class="line">...</span><br><span class="line">float4 wallXYCol = tex2D(_WallXYTexture, intersectionXY_obj) * zIsInsideBoundary;</span><br><span class="line">float4 wallZYCol = tex2D(_WallZYTexture, intersectionZY_obj) * xIsInsideBoundary;</span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/imp-interior-mapping/4.1_correct_corner.png" alt title="右边图中边缘多余的墙壁被剔除了"><h1 id="后面的改进"><a href="#后面的改进" class="headerlink" title="后面的改进"></a>后面的改进</h1><p>1.通过事先bake好光照，使室内的表现更加真实。计算真实的光照也是有可能的，只不过可能需要更多的操作，需要权衡一下是否真的需要。<br>2.处理好不同方向的偏移补偿，目前的uv offset补偿其实是有问题的，没有考虑顶部和底部的情况。（不过如果大楼是封顶的倒也不太要紧）<br>3.本文使用的是分开的墙面texture方法，也有些实现使用的是Cubemap。<img src="https://forum.unity.com/attachments/interiormapping-cubemap-jpg.198474/" alt title="图片来源：[8]"></p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://jsantell.com/model-view-projection" target="_blank" rel="noopener">[1] Model View Projection</a><br><a href="http://www.songho.ca/opengl/gl_projectionmatrix.html" target="_blank" rel="noopener">[2] OpenGL Projection Matrix</a><br><a href="https://pdfs.semanticscholar.org/8622/48de620efe27705af3702ab2a2c0d4ec76ec.pdf" target="_blank" rel="noopener">[3] Interior Mapping - A new technique for rendering realistic buildings</a><br><a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/visibility-problem-depth-buffer-depth-interpolation" target="_blank" rel="noopener">[4] The Visibility Problem, the Depth Buffer Algorithm and Depth Interpolation</a><br><a href="http://pluspng.com/png-151110.html" target="_blank" rel="noopener">[5] Test Character Sprite</a><br><a href="http://wiki.amplify.pt/index.php?title=Unity_Products:Fake_Interiors/Manual" target="_blank" rel="noopener">[6] Unity Products:Fake Interiors/Manual</a><br><a href="http://joostdevblog.blogspot.com/2018/09/interior-mapping-real-rooms-without.html" target="_blank" rel="noopener">[7] Interior Mapping: rendering real rooms without geometry</a><br><a href="https://forum.unity.com/threads/interior-mapping.424676/" target="_blank" rel="noopener">[8] Unity Forum: Interior Mapping</a><br><a href="http://interiormapping.oogst3d.net/" target="_blank" rel="noopener">[9] Interior Mapping - A new technique for rendering realistic buildings</a></p>]]></content>
      
      
      <categories>
          
          <category> Dev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> game </tag>
            
            <tag> graphics </tag>
            
            <tag> shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weekly Animation Share Vol 2</title>
      <link href="/2019/09/11/weekly-animation-share-vol-2/"/>
      <url>/2019/09/11/weekly-animation-share-vol-2/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/src/blocked-video.js"></script><h1 id="Egg-McMuffin-Director’s-Cut"><a href="#Egg-McMuffin-Director’s-Cut" class="headerlink" title="Egg McMuffin - Director’s Cut"></a>Egg McMuffin - Director’s Cut</h1><p><iframe name="iframe-blocked" src="https://player.vimeo.com/video/354894903" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/egg-mcmuffin.jpg"></iframe></p><p><a href="https://vimeo.com/354894903" target="_blank" rel="noopener">Egg McMuffin - Director&#039;s Cut</a> from <a href="https://vimeo.com/matthieubraccini" target="_blank" rel="noopener">Matthieu BRACCINI</a></p><br><a id="more"></a><br><br>这是作者Matthieu为推广麦当劳的吉士蛋麦满分而和TBWA做的广告。逼真的模拟让人看完之后真的有种非常想去点一个吉士蛋麦满分的冲动。<br>更多制作过程可以到<a href="https://www.behance.net/gallery/83279865/Egg-McMuffin-Directors-Cut" target="_blank" rel="noopener">Behance</a>查看<br><br><img src="https://mir-s3-cdn-cf.behance.net/project_modules/1400_opt_1/dc0c2d83279865.5d3db91d72f70.png" alt title="设计稿"><br><img src="https://mir-s3-cdn-cf.behance.net/project_modules/disp/5f235d83279865.5d3db91d08aa7.gif" alt title="动画WIP"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/egg-mcmuffin-jump.gif" alt title="Q弹十足的鸡蛋=w="><br><br># While You Were Sleeping<br><iframe name="iframe-blocked" src="https://player.vimeo.com/video/349486776" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/while-you-were-sleeping.jpg"></iframe><br><br><p><a href="https://vimeo.com/349486776" target="_blank" rel="noopener">While You Were Sleeping</a> from <a href="https://vimeo.com/charliestewartmotion" target="_blank" rel="noopener">Charlie Stewart</a></p><p>“Good morning!” “Good morning. I hope you slept well”…<br>一连串机械式的AI助手问答，带我们进入了这个荒芜的外太空星球里日常的一天，只不过这一次，熟悉的AI身边，缺少了些我们自己的身影…<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/while-you-were-sleeping-screenshot-1.jpg" alt title></p><p>随着AI一步步的进入我们的生活，比如Siri这样每天问候我们的AI，我们不禁思考他们到底是什么，他们和我们的关系又是什么？在漫漫的世界中，他们是否能一如既往的陪伴我们？<br>这部短片通过AI标志性的对话方式，以独特的视角来观察AI。既像是嘲讽AI那似乎永不停息却又无用的生命力，又像是哀叹人类丰富的创造力和情感，却不得不接受生命的短暂与脆弱。或许我们该问的不是AI是否能陪我们一直到最后，而是我们能否陪AI一直走下去…</p><h1 id="Negative-Space"><a href="#Negative-Space" class="headerlink" title="Negative Space"></a>Negative Space</h1><p><iframe name="iframe-blocked" src="https://player.vimeo.com/video/345922827" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/negative-space.jpg"></iframe></p><p><a href="https://vimeo.com/345922827" target="_blank" rel="noopener">Negative Space</a> from <a href="https://vimeo.com/tinyinventions" target="_blank" rel="noopener">Tiny Inventions</a></p><p>主人公Sam虽然不善言辞，打包行李却很拿手。毛巾卷成卷，衣服按规则叠好，袜子分类放在箱子顶部，一根皮带绕在箱子边缘，最后才放入皮鞋。这一切都源自他那总是出差在外的父亲。当他还是个孩子的时候就已经耳濡目染，等他稍微长大些的时候，就已经能帮助忙碌的父亲打包行李。平日里不像其他父子可以有很多时间交流，但12岁那年简单的打包却换来了父亲的一句”Perfect”。<br>时隔多年，Sam再一次打包好行李，去最后见一次教会自己这一切的那个人…</p><p>这部定格动画短片是一个基于Ron Koertge创作的150字诗。与一般的父子不同，片中的主人公与自己平日很少见面的父亲的联系则是通过打包行李来维系和一步步加深的。<br>打包的过程并不那么惊心动魄，甚至一点都不有趣，可是总是生活中这样的小细节让人感到美好。整部片子的质量很高，不仅仅是一部简单的逐帧动画，画面自然的衔接和丰富的想象力也大大的扩展了诗本来的内容。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/negative-space-screenshot-1.jpg" alt title></p><p>本片的创作者是美国的Max Porter和日本的Ru Kuwahata，他们是一对动画创作搭档，他们创办了<a href="https://www.tinyinventions.com/main/who-we-are/" target="_blank" rel="noopener">Tiny Inventions</a>，是获奥斯卡提名的动画导演，擅长用不同媒介叙事，其中以定格动画最为出名。这次的Negative Space也获得了多达127个大大小小奖项。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-2/negative-space-interview.jpg" alt title="Ru Kuwahata和Max Porter"><br>关于这部动画的创造过程，可以查看<a href="https://vimeo.com/238590794" target="_blank" rel="noopener">这里</a>。很有意思的是，Kuwahata的父亲就曾经是一位飞行员，所以他对打包行李的心得也让女儿在创作本片的时候得到了很多共鸣。</p><p><a href="https://wordsfortheyear.com/2014/12/19/negative-space-by-ron-koertge/" target="_blank" rel="noopener">Negative Space</a><br>by Ron Koertge</p><p>My dad taught me to pack: lay out everything. Put back half. Roll things that roll. Wrinkle-prone things on top of cotton things. Then pants, waist-to-hem. Nooks and crannies for socks. Belts around the sides like snakes. Plastic over that. Add shoes. Wear heavy stuff on the plane.</p><p>We started when I was little. I’d roll up socks. Then he’d pretend to put me in the suitcase, and we’d laugh. Some guys bond with their dads shooting hoops or talking about Chevrolets. We did it over luggage. By the time I was twelve, if he was busy, I’d pack for him. Mom tried but didn’t have the knack. He’d get somewhere, open his suitcase and text me—“Perfect.” That one word from him meant a lot.</p><p>The funeral was terrible—him laid out in that big carton and me crying and thinking, Look at all that wasted space.</p>]]></content>
      
      
      <categories>
          
          <category> Motion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> anime </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weekly Animation Share Vol.1</title>
      <link href="/2019/09/07/weekly-animation-share-vol-1/"/>
      <url>/2019/09/07/weekly-animation-share-vol-1/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/src/blocked-video.js"></script><p>作为第一期的每周动画分享（会有几期也不知道…），先简单介绍下做这个系列的动机。之前经常会逛<a href="http://animetaste.net/" target="_blank" rel="noopener">AT! 品赏阿尼墨</a>，通过这个平台看到过不少优秀的作品。但因为网站已经很久没更新了，而如今的动画公众号也缺少一些独立短片的推荐，所以出于个人兴趣爱好，决定（不）定期的更新一些自己发现的有意思的动画短片。这次的三个视频都来自vimeo，由于被墙了，所以国内小伙伴可能看起来不太方便，目前只好提供截图封面，后面我会思考下如何解决这个问题…</p><h1 id="CAT-DAYS-ねこのひ"><a href="#CAT-DAYS-ねこのひ" class="headerlink" title="CAT DAYS / ねこのひ"></a>CAT DAYS / ねこのひ</h1><p><iframe name="iframe-blocked" src="https://player.vimeo.com/video/296302547?color=ef1900&title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-1/cat-days.jpg"></iframe></p><p><a href="https://vimeo.com/296302547" target="_blank" rel="noopener">CAT DAYS / ねこ の ひ / Neko no Hi</a> from <a href="https://vimeo.com/user45689947" target="_blank" rel="noopener">Jon Frickey</a></p><a id="more"></a><p>这一部2018年的动画获得过多个动画节的大奖，讲述的是一个叫Jiro的小男孩在某一天得了一场病。当他爸爸带他去看病的时候，医生带给了他们一个“惊人”的消息，男孩竟然可能是猫。在接下来等待最终鉴定结果的几天里，男孩和爸爸都试图接受这个身份的转变…<br>这部片子很容易让人联想到细田守的动画<a href="http://www.ookamikodomo.jp/index.html" target="_blank" rel="noopener">《狼的孩子雨和雪》(おおかみこどもの雨と雪)</a>，同样是讲述的人的动物化。他们之所以能将生活中本不可能真实发生的事，描绘的非常自然，得益于他们能将身份认同/身份代入这一点用巧妙的方式代入到故事的主体中。人固然是人本身，可是其他动物的天性同样反映在人的身上，尤其是当一个人还是个孩子的时候。通过将人和动物的双重形象进行对比，身份的转变，孩子的成长这一主题得以通过一个更为突出的过程体现了出来。<br>这样一部日系满满的动画，没想到竟然出自一位来自德国的40岁的大叔动画导演<a href="https://www.jonfrickey.com/about" target="_blank" rel="noopener">Jon Frickey</a><br><img src="https://m.media-amazon.com/images/M/MV5BM2Q0N2IzY2ItNWEwYy00NGU3LWIxNTEtM2Y3MDk0Y2ExMjc0XkEyXkFqcGdeQXVyODI5NTk1NzI@._V1_UY317_CR11,0,214,317_AL_.jpg" alt="Jon Frickey"></p><h1 id="DOOOOUGH-YEAH"><a href="#DOOOOUGH-YEAH" class="headerlink" title="DOOOOUGH YEAH!"></a>DOOOOUGH YEAH!</h1><p><iframe name="iframe-blocked" src="https://player.vimeo.com/video/236225252?color=ef1900&title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-1/doooough-yeah.jpg"></iframe></p><p><a href="https://vimeo.com/236225252" target="_blank" rel="noopener">DOOOOUGH YEAH!</a> from <a href="https://vimeo.com/wesleyfuh" target="_blank" rel="noopener">Wesley Fuh</a></p><p>这是一个有趣的动画短片，讲述的是一个诞生于边角料的瘦小面团，看到其他的面团经过烘焙都成为了“肌肉猛男”(beautiful bread 笑)，迫不及待的想跻身他们的行列，结果机缘巧合之下，变成了椒盐卷饼(brezel)<br>这个动画的亮点在于很有创意。将烤熟的菠萝面包想象成有六块腹肌的健身猛男，而涂上的黄油酱更是成为了耀眼的金光，难怪没有一个面团甘于平庸。幸好结果是正面的，因为边角料面团成为了更beautiful的存在（。<br>另外，该片的声效似乎都是作者自己配的，倒是很可爱。最后，大家可能都会记住那一声声的”Dooough Yeah”了吧</p><h1 id="The-Year-of-the-Pig"><a href="#The-Year-of-the-Pig" class="headerlink" title="The Year of the Pig"></a>The Year of the Pig</h1><p><iframe name="iframe-blocked" src="https://player.vimeo.com/video/315958054?color=ef1900&title=0&byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen altimgurl="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/weekly-animation-share-vol-1/the-year-of-the-pig.jpg"></iframe></p><p><a href="https://vimeo.com/315958054" target="_blank" rel="noopener">The Year of the Pig</a> from <a href="https://vimeo.com/ryanbooth" target="_blank" rel="noopener">Ryan Booth</a></p><p>这部小短片严格意义上讲并不好分类。从作者的简单介绍中，他也形容它为”just a collection of moments…”。<br>短片记录的是纽约庆祝农历猪年时的场景以及生活中的一些画面，全片平淡却又有让人感动的小细节，宛如一个人在静静的回忆温暖的瞬间。整体的画面经过电影调色，体现出一种冬日里一股温暖的感觉，也颇有中国80,90年代电影的感觉。短片中的相机抖动既是转场的需要，也为画面增加了不少生活的真实感。<br>不同于中国人庆祝新年时惯用的喜庆音乐，全片的背景音乐都非常祥和而安静，让我们这些来自中国的人们感到有些好奇和陌生。新鲜之余，也从中能窥探出不同文化的人们对未来期待的不同表达。<br>带给我类似感受的还有<a href="https://www.imdb.com/title/tt0401711/" target="_blank" rel="noopener">《巴黎，我爱你》(Paris, je taime)</a>，同样没有过于复杂的剧情，更多的是纪录片似的喃喃自语</p>]]></content>
      
      
      <categories>
          
          <category> Motion </category>
          
      </categories>
      
      
        <tags>
            
            <tag> anime </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes on ddx/ddy</title>
      <link href="/2019/08/25/Notes-on-ddx-ddy/"/>
      <url>/2019/08/25/Notes-on-ddx-ddy/</url>
      
        <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>ddx/ddy用于返回屏幕空间中某一值关于x,y方向上的偏导数。它只可以用于fragment program中，参数必须来自fragment的输入。<br>HLSL对<a href="https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-ddx" target="_blank" rel="noopener">ddx的定义</a>：Returns the partial derivative of the specified value with respect to the screen-space x-coordinate.<br>GLSL中对应的函数为<a href="https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/dFdx.xhtml" target="_blank" rel="noopener">dFdx/dFdy</a></p><a id="more"></a><h1 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h1><p>在triangle rasterization阶段，GPU会对多个像素实例同时运行fragment shader。在最细的程度上，它将会通过SIMD同时跑32-64个像素（实例）。在这些像素中，又会被划分成2 * 2的小组（又被称为quad-fragments），所以每个组会有四个相邻的像素，这样SIMD指令就能同时处理屏幕上2 * 2 = 4个像素了。<br>每个像素中的值都来自于vertex program或其他步骤输出之后得到的插值，所以对于这些值来说，在进入到fragment shader之前就完成了计算，这些是可以并行计算得到的。所以在fragment shader中运行ddx(vertexOuput)的时候，就能知道这个插值的变化导数。</p><p>fragment shader之所以能够访问到相邻像素的数据（即SIMD的不同lanes/thread，每个lanes/thread都处理一个像素），是因为GPU利用了(quad) swizzle进行了跨lanes的数据访问<br><img src="https://anteru.net/images/2018/pixel-crosstalk.svg" alt title="来源：[7]"></p><p>对于AMD的GCN，<code>ds_swizzle_b32</code>指令的offset field是留给<code>ds_pattern</code>的。它有两种模式<a href="#ref">[10]</a>：</p><blockquote><ul><li><strong>Quad-permute mode (QDMode)</strong>: Each of the four adjacent lanes can access each other’s data, and the same switch applies to each set of four. The ds_pattern LSBs directly encode the element ID for each lane.</li><li><strong>Bit-masks mode (BitMode)</strong>: This mode enables limited data sharing within 32 consecutive lanes. Each lane applies bitwise logical operations with constants to its lane ID to produce the element ID from which to read. Constants are encoded in ds_pattern .</li></ul></blockquote><p><img src="https://gpuopen.com/wp-content/uploads/2016/08/ds_pattern.png" alt title="对应的图示"><br>QDMode模式会更清晰一点<br><img src="https://gpuopen.com/wp-content/uploads/2016/08/qdmode.png" alt title="一个简单的例子：分别将原来的0,1,2,3位变成2,1,3,3位"><br><img src="https://gpuopen.com/wp-content/uploads/2016/08/ds-swizzle-b32.png" alt title="过程示意图"></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="1-计算Lod或各向异性-Anisotropy"><a href="#1-计算Lod或各向异性-Anisotropy" class="headerlink" title="1.计算Lod或各向异性(Anisotropy)"></a>1.计算Lod或各向异性(Anisotropy)</h2><p>因为mipmap level需要计算fragmentInput.uv的导数，即相邻像素之间对应纹素的差距。过程是将屏幕坐标的点映射到uv平面上，然后计算出距离。取出最大的一个距离L，并做$log_2⁡L$。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/ddx/mipmap_lod.png" alt title="计算mipmap level图示，来源：[5]"><br>其含义就是如果uv平面中采样点很稀，相应的du/dx或其他的值就比较大，L也相应的比较大。这种情况说明Minification了，很多纹素就堆在了一个像素里。我们实际上用不到这么多纹素，于是一个很高级别的mipmap(更模糊的）就可以用来节省渲染的负担，也可以避免出现锯齿。<br><img src="https://i.stack.imgur.com/VJEqW.png" alt title="计算uv的偏导的图示，来源：[1]"></p><h2 id="2-计算面法线"><a href="#2-计算面法线" class="headerlink" title="2.计算面法线"></a>2.计算面法线</h2><p>在 FragShader 中，调用<code>ddx(i.position)</code>, 和<code>ddy(i.position)</code>可以求出相邻的2 个像素之间座标的差值，即两个像素在三角面上采样的点所构成的向量。下面图中的红色和绿色2个向量即为ddx,ddy所返回的向量<br><img src="http://album.sina.com.cn/pic/002hBfPnzy7dvLft6NX99" alt title="来源：[2]"></p><p>而这2 个向量都在这个三角形的平面上，那么<br><code>normal = normalize(cross(ddx(pos), ddy(pos)))</code><br>就可以求出的面的法线，但是这里要注意，在 HLSL 或者Unity shader里要写成normalize(cross(ddy(pos), ddx(pos))) , 不然法线是反向的。这个是由于左右手座标系引起的。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>嵌套使用ddx(p)或ddy(p)可能导致undefine的值，如ddx(ddx(p))或ddx(ddy(p))。<br>同时，程序假设参数p是连续的。对于嵌在flow-control代码段的值，因为不是每一个在quad-fragment中的片段都会执行同样的branch，所以结果也是undefine的<br>ddx/ddy分为coarse版本和fine版本。如果直接使用ddx，等价于ddx_coarse（可以看做alias).<br><a href="https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/ddx-coarse" target="_blank" rel="noopener">ddx_coarse</a>和<a href="https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/ddx-fine" target="_blank" rel="noopener">ddx_fine</a>对于同一组2*2的quad-fragment的四个像素，他们的偏导数都将会是一样。这两个函数需要Shader Model 5的支持<a href="#ref">[4]</a></p><p>其他相关函数：<code><a href="https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-fwidth" target="_blank" rel="noopener">fwidth(p)</a> = abs(ddx(p)) + abs(ddy(p))</code></p><p><span id="ref"></span></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://gamedev.stackexchange.com/a/130933" target="_blank" rel="noopener">[1] What are screen space derivatives and when would I use them?</a><br><a href="http://www.aclockworkberry.com/shader-derivative-functions" target="_blank" rel="noopener">[2] An introduction to shader derivative functions | A Clockwork Berry</a><br><a href="https://gamedev.stackexchange.com/a/62650" target="_blank" rel="noopener">[3] What does ddx (hlsl) actually do?</a><br><a href="https://fgiesen.wordpress.com/2011/07/10/a-trip-through-the-graphics-pipeline-2011-part-8/#comment-1990" target="_blank" rel="noopener">[4] A trip through the Graphics Pipeline 2011, part 8</a><br><a href="http://15462.courses.cs.cmu.edu/spring2018/lecture/persp/slide_055" target="_blank" rel="noopener">[5] [CMU 15462] Lecture 7: PerspectiveTexture</a><br><a href="https://catlikecoding.com/unity/tutorials/advanced-rendering/flat-and-wireframe-shading/" target="_blank" rel="noopener">[6] Flat and Wireframe Shading Derivatives and Geometry</a><br><a href="https://anteru.net/blog/2018/more-compute-shaders/" target="_blank" rel="noopener">[7] More compute shaders</a><br><a href="https://developer.nvidia.com/gpugems/GPUGems2/gpugems2_chapter35.html" target="_blank" rel="noopener">[8] [GPU Gems 2] 35.2.4 The Swizzle Operator</a><br><a href="https://developer.nvidia.com/reading-between-threads-shader-intrinsics" target="_blank" rel="noopener">[9] [Reading Between The Threads: Shader Intrinsics] Fragment Quad Swizzle - Data Exchange and Arithmetic</a><br><a href="https://gpuopen.com/amd-gcn-assembly-cross-lane-operations/" target="_blank" rel="noopener">[10] [AMD GCN Assembly: Cross-Lane Operations] The Swizzle Instruction</a></p>]]></content>
      
      
      <categories>
          
          <category> Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> graphics </tag>
            
            <tag> shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作不使用贴图的水滴特效</title>
      <link href="/2019/07/31/raindrop-effect/"/>
      <url>/2019/07/31/raindrop-effect/</url>
      
        <content type="html"><![CDATA[<p>Sorry, currently unavailable…</p><p>Hopefully, it will come back soon ;)</p>]]></content>
      
      
      <categories>
          
          <category> Dev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> graphics </tag>
            
            <tag> shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从像素之间谈起：像素游戏的画面增强（下）</title>
      <link href="/2017/07/03/from_pixel_to_screen_2/"/>
      <url>/2017/07/03/from_pixel_to_screen_2/</url>
      
        <content type="html"><![CDATA[<p>上篇见 <a href="/2017/07/02/from_pixel_to_screen_1/">从像素之间谈起：像素游戏的画面增强（上）</a></p><h1 id="其他可能的改进"><a href="#其他可能的改进" class="headerlink" title="其他可能的改进"></a>其他可能的改进</h1><h2 id="投影增强"><a href="#投影增强" class="headerlink" title="投影增强"></a>投影增强</h2><p>前面我们在进行扩散投影模拟的时候，是同时对周围八个点进行采样，但是事实上，有时为了控制投影的方向，可以只对一侧的点进行采样<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/sample_weight_partial.png" alt title><br><a id="more"></a><br>如图所示，只需要对右下角的五个格子采样，就可以模拟出左上角的光照。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/unilateral_sample.png" alt title><br>这样造成的效果是亮的部分会凸起，暗的部分会产生凹陷的效果<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/darkness_offset.png" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/lightness_offset.png" alt title></p><h2 id="函数的拟合"><a href="#函数的拟合" class="headerlink" title="函数的拟合"></a>函数的拟合</h2><p>前面在计算相邻点的加权颜色值时，用到了一个指数函数。指数函数的效果的确很好，考虑到在某些平台上exp的消耗可能有点大。另外，任意两个像素之间的欧几里得距离不会超过2.3个像素，所以我们尝试对函数进行一个拟合，如0.926+1.441x + 0.6578x^2 + 0.0417x^4<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/approx_1.jpg" alt title><br>其实我们还可以将把它化成1/(7x^2 +1)，效果也还可以，只是无论是哪种情况，在PC上测试差距并不明显（也有可能适得其反）<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/approx_2.png" alt title></p><h2 id="扫描线"><a href="#扫描线" class="headerlink" title="扫描线"></a>扫描线</h2><p>考虑到有些游戏中，会出现一些因为曝光过度而无法显示扫描线的情况。于是，我们就需要对扫描线进行加强：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> limit = <span class="number">1</span> - step(<span class="number">257.0</span>, min(frac(i.pixel_no.x), <span class="number">1</span>- frac(i.pixel_no.y)) * _MainTex_TexelSize.z);</span><br><span class="line"><span class="keyword">float</span> bright = Luminance(out_color);</span><br><span class="line"><span class="keyword">return</span> fixed4(out_color *(<span class="number">1.8</span> - limit * bright * bright * <span class="number">0.89</span>), <span class="number">1.0</span>);</span><br></pre></td></tr></table></figure></p><p>但是对于某些偏暗的游戏，如果为了提高整体亮度，而扫描线同时也强化的很厉害，那么就会导致“碳化”<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/Overexposed.png" alt title="中间的白色由于亮度过高，在补偿的时候会显得非常暗"><br>虽然这样的扫描线加强在其他场合正是我们需要的，但在这里只会让画面变得很脏。关于这个问题并没有很好的解决方案，这需要根据不同游戏对参数做出调整。作为游戏开发，如果美术风格及早的确定，颜色的选择有所参照，将会对程序的优化有极大的帮助。而2D像素游戏由于很少受光照影响，再加上像素画本身也极其依赖于palette，所以如果palette控制的好，是可以根据其调试出一个很好的状态的。</p><p>由于目前只用针对一款游戏，所以上面的手工调整可以接受。如果我们需要大量的调整，我在想，可能还有一种思路是像tone mapping一样，将亮度映射在一个合理的区域内，这样既保留了细节又处理了边界状况。</p><h2 id="Tactics-Ogre的特殊处理"><a href="#Tactics-Ogre的特殊处理" class="headerlink" title="Tactics Ogre的特殊处理"></a>Tactics Ogre的特殊处理</h2><p>刚开始我为Taactic Ogre（中文译为：皇家骑士团）写shader的时候，出现了一个问题。由于很容易知道psp的分辨率是480*272，我就将其硬编码到shader中。但是却出现了一些意想不到的状况。在横坐标方向上，扫描线的分布不均匀。由于是周期性的，并且随着窗口的扩大问题更为严重，我最开始猜测是模拟器的精度出现了问题，我查了下changelog也的确提到了这个问题，只是我使用的版本应该已经修复了这个问题。另外，我测试了其他的游戏，发现一切都很正常，如果真的是精度问题，不该只出现在这一款游戏上。查看了整个render过程后发现Tactics Ogre中有些地方与其他游戏做的不同，比如<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/ToScreen5.png" alt title="注意纹理右侧的黑边"><br>Tactics Ogre在draw顶部的滚动文本时，并不会对其裁剪，而是放到了第二个color pass里才进行裁剪<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/psp_firstpass_cut.png" alt title><br>不过ppsspp模拟器提供了u_texelDelta这样一个uniform，我们可以利用它得知当前输入纹理的resolution：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vec2 c_resolution = <span class="number">1.0f</span> / u_texelDelta;</span><br></pre></td></tr></table></figure></p><p>这样，即使在某些场景中，屏幕的分辨率发生变化，我们也能够保证显示正确的扫描线。<br><br></p><h1 id="最终PSP模拟器效果图"><a href="#最终PSP模拟器效果图" class="headerlink" title="最终PSP模拟器效果图"></a>最终PSP模拟器效果图</h1><p>在这里给出自己制作的在PSP模拟器上的最终效果，请放大后观察<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To1.png" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To2.jpg" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To3.jpg" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To4.jpg" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To5.jpg" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/zipped_To6.jpg" alt title><br><br></p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>本文主要讨论了针对细像素游戏的画质增强，但是这并不意味着像素游戏的增强方式只有一种，相反，光是<a href="http://filthypants.blogspot.jp/2015/04/more-crt-shaders.html" target="_blank" rel="noopener">这里</a> 就提到多种后期特效。我们也无法说哪种效果比另一种更好。更多的时候，还是需要根据对游戏的定位来定制自己的后期特效，从而让画面为游戏核心服务。程序和美术之间的沟通是否充分也是能否有效的构建出成功的游戏画面中很重要的一个因素。</p><p>最后，你们觉得这是一篇讨论像素游戏中画面增强的文章吗？<br>不，不是的，我只是在安利Tactics Ogre :P</p><p>另：为防止图床炸裂，请勿随意转载:)</p>]]></content>
      
      
      <categories>
          
          <category> Dev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> game </tag>
            
            <tag> graphics </tag>
            
            <tag> shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从像素之间谈起：像素游戏的画面增强（上）</title>
      <link href="/2017/07/02/from_pixel_to_screen_1/"/>
      <url>/2017/07/02/from_pixel_to_screen_1/</url>
      
        <content type="html"><![CDATA[<p>由于文章太长，我将最初的文章拆成两个部分，下篇见 <a href="/2017/07/03/from_pixel_to_screen_2/">从像素之间谈起：像素游戏的画面增强（下）</a></p><h1 id="无所不在的像素画"><a href="#无所不在的像素画" class="headerlink" title="无所不在的像素画"></a>无所不在的像素画</h1><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>随着分辨率的普遍提高，我们已经告别了依赖于简陋像素来表现游戏画面的年代。但还是有不少人像我一样沉迷于像素美术和游戏。如今到处可以都可以看到的各式像素作品，虽然大多被直接称呼为像素画，但实际上已经分化为很多分支，简单的将其归类为像素作品未免太含糊。在开始正文之前我先将他们粗粗的分个类。一些比较常见的代表如：</p><ol><li>大颗粒像素，此类像素作品一般细节较少，人物符号化或者抽象化。同时还可能出现非像素元素，如光晕，渐变 <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/large_scale_pixel.jpg" alt title="单键Bob，一个颇为爽快的flash游戏"><a id="more"></a></li><li><p>粒度较小的像素画，主要还是色块化，边缘并没有强化。 <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/dragon_den.gif" alt title="HGSS中的Dragon Den"></p></li><li><p>强化边缘和高光，细节丰富，但是普遍尺寸较小。 <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/small_scale_pixel.png" alt title="Drill Dozer截图"></p></li></ol><p>另外，在一些UI图标的绘制过程中，由于图标较小，也同样采用像素点绘的方式。因为它平时也不会被称为像素画，所以这里也不讨论。</p><p>其中第3种是我在本文中将着重讨论的。<br>这类像素图可能和平时所提到的像素图差的最远，因为它并不是为了做出像素化效果而诞生的。相反它是游戏机在分辨率和色板支持加强之后的产物（光是从GB到GBC，支持的色深就从2位变成了15位）。在这方面，任天堂算是是做到了极致（也可能因为任天堂的主机的屏幕天生小的缘故）这类像素画在抗锯齿（伪），光照，色彩的调和的方面很有特点（这篇文章中不细说）</p><h2 id="再现像素画"><a href="#再现像素画" class="headerlink" title="再现像素画"></a>再现像素画</h2><p>就GBA而言，分辨率为240 *160，但我们现在再制作像素的游戏时，玩家一定不会接受在这么小的一个屏幕上去玩游戏。一个是因为眼睛看的太累（长大后眼睛都变差了…）。另一方面，考虑到像素画的成本，也不建议针对一个1080p的屏幕进行逐像素绘画。为了满足一些玩家想要的像素的效果，一个最简单直接的方法就是将画面放大。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/small_scale_pixel_zoomed.png" alt title="这幅图放大了3倍之后，也许会更容易于将它认为是像素画风"></p><p>虽然这种方法省时省力，但是也会带来一个问题。在绘制像素画中的曲线时，由于一般不对线条使用反走样（会让画面变脏）来抗锯齿。在分辨率较低的时候，像素的边缘可以帮助人们识别且很难注意到异样，但当画面放大后，这些边缘就会显得粗糙不堪。这也是像素画风被一些人所诟病的原因。</p><p>为此，包括ppsspp在内的模拟器中，会内置不少shader来对图像进行后期处理。对于2D图像来说，具体方法包括xBRZ等滤镜来平滑放大图像（xBRZ对2D像素放大会产生平滑而舒适的效果，但是这会损失像素的特征），增加crt, 扫描线等后期特效将像素画做旧。当然，你也可以利用物理的手段将信号输出到CRT屏幕上，参考<a href="http://wavebeam.blogspot.jp/2016/01/a-beginners-guide-to-best-retro-gaming.html" target="_blank" rel="noopener">这里</a><br>另外，<a href="https://blz.la/rgb/gaming_crt.html" target="_blank" rel="noopener">这篇文章</a> 中讲述了一些crt效果的来源，也讨论了很多细节问题。一个简单的对比图： <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/metal_slug.jpg" alt title="from: http://www.neogaf.com/forum/showthread.php?p=236239524"><br>常见的效果如下<br><img src="http://i58.tinypic.com/2lnu2b9.png" alt="尝试翻墙显示" title="from: http://shmups.system11.org/viewtopic.php?f=6&t=51298&start=30" width="50%"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/geom.jpg" alt title="注意屏幕的扭曲，这其实是crt的物理性质决定的"><br><img src="http://abload.de/img/royale0iu4f.png" alt="尝试翻墙显示" title="注意像素的膨胀" width="50%"></p><p>虽然实现方法不同，但总的来说都是在像素之间增加了隔断，人们的大脑会趋向为这种断裂解释理由，自动为图像进行内部平滑处理。这就和我们凑近屏幕看游戏画面但是不会觉得画面模糊的原因类似。另一方面，因为扫描线的存在，画面的层次感也可以体现出来，使得画面更加可信。甚至连Her Story中都为了剧情的需要用些crt效果。<br><a href="http://filthypants.blogspot.jp/2015/04/more-crt-shaders.html" target="_blank" rel="noopener">这篇文章</a>里介绍了大量的post processing shader，很有借鉴意义。<br><br></p><h1 id="一个shader的实现思路"><a href="#一个shader的实现思路" class="headerlink" title="一个shader的实现思路"></a>一个shader的实现思路</h1><p>本文的后期特效将主要适用于前面所述的第三种情况，也即通过临近采样的方式放大图像而达到加强像素化的目的。更多的模拟LCD屏幕而不是CRT屏幕，所以一些包括屏幕扭曲，通道分离的效果在本文中将不会涉及。本文会利用psp模拟器，将扫描线效果应用到Tactics Ogres（中文译为：皇家骑士团）上。<br>我主要从两方面完成对像素图的画面增强：1.利用微小的分割线来分隔开像素，让人们产生像素相连的错局。2.利用低通滤波器稍许的平滑像素边界（但是不宜平滑太多，不然会失去像素风格的特点）</p><p>为了统一，后面的演示代码都用CG来写，输入的纹理尺寸为512 x 384</p><h2 id="格子的分割"><a href="#格子的分割" class="headerlink" title="格子的分割"></a>格子的分割</h2><h3 id="硬分割"><a href="#硬分割" class="headerlink" title="硬分割"></a>硬分割</h3><p>首先，将像素放大了2倍之后，实际看到的一个“像素 pixel”（叫纹素 texel更为贴切）是2 x 2个像素。虽然我们想营造出的效果是让玩家觉得游戏的像素与像素之间产生了间距，但除了在原先的一个像素上通过勾画边缘来实现分割，我们并不能真的将像素之间创造出空格。这步操作之后，最小单位仍然是像素。下图所示的分别是每2个像素进行一次分割和每4个像素进行一次分割的图示。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/hard_separate_1.png" alt title="每两格有一个明暗变化周期"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/hard_separate_2.png" alt title="每四格有一个明暗变化周期"><br>对于后期特效来说，输入的纹理为camera input，上图是1 texel对应 4 pixel，而下面是1 texel对应 16 pixel。<br>为了找到分割的位置，需要能够区分一个纹素所对应的像素。方法并不复杂, 若一个纹素拆分为4*4个像素，可以在顶点着色器上输出如下vec2：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o.pixel_no = float2(o.uv.x * _MainTex_TexelSize.z, o.uv.y * _MainTex_TexelSize.w) * <span class="number">0.25</span>;</span><br></pre></td></tr></table></figure></p><p><code>_MainTex_TexelSize</code> 是内置uniform，记录输入纹理的相关信息，其中zw分量即为宽和高。对于ppsspp模拟器，可以通过 <code>u_texelDelta</code> 来计算屏幕的resolution，后面会提到。<br>有了pixel_no的信息，我们就可以在片段着色器里进行插值了：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">fixed4 <span class="title">Pass_Scanline</span><span class="params">(float2 uv)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">float</span> column = <span class="number">4</span>;</span><br><span class="line">        float2 pixel_no = </span><br><span class="line">            frac(float2(uv.x * <span class="number">1024.0</span>, uv.y * <span class="number">768</span>) * _ScreenScale / column);</span><br><span class="line">        <span class="keyword">if</span>(pixel_no.x &lt; <span class="number">1</span> / column || pixel_no.y &lt; <span class="number">1</span> / column)</span><br><span class="line">            <span class="keyword">return</span> PREVIOUS_PASS(uv) * <span class="number">0.5</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> PREVIOUS_PASS(uv);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中PREVIOUS_PASS是一个宏，用来嵌套伪multi-pass，这里的PREVIOUS_PASS可以简单的理解为上一个获取纹理的值的pass。这里当column为4的时候，一个纹素对应的四个像素的pixel_no的x分量分别为1/8, 3/8, 5/8, 7/8，我们可以利用这个信息来判断究竟哪个像素是这个纹素的边缘。<br>硬分割虽然完成了对像素的分割，但是效果比较生硬。玩家感受到的不是从屏幕上反映的图像，而更像是罩上了网格的图像。这也和asset store上的<a href="https://www.assetstore.unity3d.com/en/#!/content/73708" target="_blank" rel="noopener">这个效果</a>类似。</p><h3 id="丰富分割细节"><a href="#丰富分割细节" class="headerlink" title="丰富分割细节"></a>丰富分割细节</h3><p>硬分割的效果不理想，于是很自然的想到为这个边缘添加一些过渡效果是否会好一点呢？答案是肯定的。另外，为了能取得比较好的过渡效果，我们应该适当提高pixel对texel的比例，测试下来发现一般来说3比较合适，2的话太窄，而4的话，图像放大的过大。<br>为了理解方便，我们将图像的边缘定义为暗，图像的中央定义为亮，这样明暗间隔就能产生所谓的扫面线。问题演变为在一个纹素所对应的所有像素中，如何找到一个亮与暗的分布，从而表现出一个荧光格子的效果<br>如果单纯的亮度从中心开始，依照切比雪夫距离向边缘递减，效果其实不太理想，纹素与纹素之间割裂的依旧生硬<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/cos_sum_tween.png" alt title><br>所以我们想找到一种方式柔滑这一过程，首先可以尝试用高斯平滑来处理<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/gaussian_smooth.png" alt title="不过作用效果还是在一个纹素内，所以还是不够好"></p><p>卷积核<br>简单的过渡不够，所以需要找到一个卷积核（kernel）来将像素周围的情况考虑进去，最常见的低通滤波器就是高斯滤波器（Gaussian Filter）但直接使用的话，会造成画面均匀平滑。Themaister提供了一个很好的思路（虽然由于git目录失效，原始的代码已经不可考，但是我还是在网上找到了一个<a href="https://searchcode.com/codesearch/view/26809099/" target="_blank" rel="noopener">GLSL版本</a> ），效果如下图所示：<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/dotnbloombig.png" alt title="除了有些恼人的小黑边，但是总体效果非常接近我想要的最终效果"><br>他的思路简单概述起来就是，一组像素（如4x4）向所在纹素的相邻8个纹素取样，权重为该像素到纹素距离倒数的负相关。本质上是一个非对称的低通滤波器。它的优势在于，针对每个纹素内的像素，所采样的纹素是一致的（保留了像素的质感）而在纹素内部，利用非对称的卷积核实现亮度的变化。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/9pixel_neighbours.png" alt title="一个纹素被分为9个像素"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/sample_weight.png" alt title="取左上角的像素进行演示，红色的线条的长度与权重成负相关"><br>我们知道越靠近中间，加权值越高，对于一个靠左下角的像素来说，将其卷积核画出来可能会像这样：<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/euclid_kernel_squared.jpg" alt title="权重为Exp(-2.05 * 平方欧氏距离)"><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/euclid_kernel.jpg" alt title="权重为Exp(-2.05 * 欧氏距离)"><br>之所以不选择平方欧氏距离，是因为这会造成加权之后，中间亮度区分不开来，而周围的亮度又太低，会有种硬分割的感觉。<br>在对周围的采样做了积分之后可以得到下图。虽然和前面的图很像，这张图的意义和刚才的并不一样，它代表的是一个纹素内的亮度分布（假设亮度的原始分布均匀）。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/low-pass_filtering.jpg" alt title><br>考虑到以上的操作局限在一个很小的范围内，所以我们可以将其离散化后观察<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/after_discretization.jpg" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/top_view_discretization.jpg" alt title="从顶部看会更直观"></p><h3 id="一些细节"><a href="#一些细节" class="headerlink" title="一些细节"></a>一些细节</h3><h4 id="滤波器的构成"><a href="#滤波器的构成" class="headerlink" title="滤波器的构成"></a>滤波器的构成</h4><p>Themaister的方法中，考虑了亮度对像素最终颜色的影响，这个滤波器由两个函数构成，一个是空间域上的滤波器系数，另一个是值域（亮度）上的系数。如果采样点上的亮度越亮，意味着它将会更多的侵蚀着其他的像素。有关Glow效果，可以参考<a href="http://www.gamasutra.com/view/feature/130520/realtime_glow.php" target="_blank" rel="noopener">这篇文章</a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">color_bloom</span><span class="params">(float3 color)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// const float3 gray_coeff = float3(0.30, 0.59, 0.11);</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> shine = <span class="number">0.25</span>;</span><br><span class="line">    <span class="keyword">float</span> bright = Luminance(color);<span class="comment">//dot(color, gray_coeff);</span></span><br><span class="line">    <span class="keyword">return</span> lerp(<span class="number">1.0</span> + shine * <span class="number">0.5</span>, <span class="number">1.0</span> - shine, bright);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里我们除了可以自己定义gray_coeff以外，我们也可以使用unity中的内置函数，它对应的 <code>gray_coeff</code> 为fixed3(0.22, 0.707, 0.071)<br>另外，通过在lerp的时候增加一个系数，我将暗部的亮度稍微提高了下，弥补曝光不足的情况。</p><h4 id="No-的偏移"><a href="#No-的偏移" class="headerlink" title="No.的偏移"></a>No.的偏移</h4><p>刚才的卷积核只是一个理想状态的演示，实际上，由于任意两个纹素是相邻的，所以只能在一个纹素的两边（看成一个正方形）上进行边的绘制。否则，两个相邻纹素在交界处都绘上黑边会导致扫描线过粗。另外，如果直接采样，将会出现平顶的情况，也即是当边上为偶数个像素的时候，中间会出现高度一样的状况。于是需要对之前的pixel_no进行偏移，偏移之后将会打破原有的平衡，找到一个新的中心。这里的偏移值应该小于1/(column * 2)，否则循环周期将会出问题。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> delta = dist(frac(pixel_no + float2(<span class="number">-0.125</span>, <span class="number">0.125</span>)), offset + float2(<span class="number">0.5</span>, <span class="number">0.5</span>));</span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/before_offset.jpg" alt title> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/after_offset.jpg" alt title><p>通过对比可以看出，偏移之后，左侧和上侧的亮度明显变暗，亮度会表现的更集中在中间的一个点。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/different_subdivision_level.png" alt title="图所示为不同粒度下的表现"></p><h4 id="采样的偏移"><a href="#采样的偏移" class="headerlink" title="采样的偏移"></a>采样的偏移</h4><p>为了给物体增加一些投影，特别是文字，会对当前像素点的周围采样。我们并不是直接用相邻像素采样（相邻像素很有可能来自于同一纹素，所以采样没有意义），而是偏移一段距离，这和ps中的投影是一个原理。只是这里需要特别注意一个问题，也即是之前看到的一张图中出现的黑边问题。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/dotnbloombig.png" alt title="注意人物轮廓周围的小黑边"><br>这个问题的起因是：如果采样点之间始终距离为一个纹素的时，虽然能保证取到的都是周围的纹素，但当图像中文本的边界正好是处于格子的边缘（也就是亮度最低的位置）在经历一个周期后，亮度是最低的地方（周期性所致）就会对之前还在暗色边界范围内的像素采样，这样就会出现在一个白的背景上出现了一条黑边。<br>解决方法就是将采样偏移限制在纹素所包含的像素个数之内，虽然这意味着我们的投影无法超过一个纹素，但是起码会避免一些比较糟糕的情况。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/problem_of_blackline.png" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/problem_of_blackline_solved.png" alt title></p><h4 id="欧氏距离与曼哈顿距离的选择"><a href="#欧氏距离与曼哈顿距离的选择" class="headerlink" title="欧氏距离与曼哈顿距离的选择"></a>欧氏距离与曼哈顿距离的选择</h4><p>前面在谈到权重的时候，我们的图示标注出来的是欧几里得距离，那么如果为了将指令减少几条，变成曼哈顿距离如何呢？结果是：并不好<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/euclid_dist.jpg" alt title="可以看出，形成了一个明显的十字亮斑，并且高度差异并区分度不高"><br>另外值得一提的是，由于编译器和显卡的优化，使用曼哈顿距离并不能节省什么开销。<br><br></p><h2 id="增加bloom"><a href="#增加bloom" class="headerlink" title="增加bloom"></a>增加bloom</h2><p>Bloom能起到加光晕的效果，能进一步降低粗糙感。通常来说，bloom只是作为HDR的一环，过程还可以包括Tone Mapping、Bright Pass Filter以及Blur。但由于我们这里只考虑2D的情况，更多时候HDR可以由美术手工实现，所以我们先不讨论ToneMapping而简单实现Bright和Blur。</p><p>1    混合横向的bloom和纵向的bloom<br>比较常见的bloom中的blur过程分为两次，一次横向像素上的模糊，一次纵向像素上的模糊，两次叠加。但是我们为了省力，也可以在一个pass中进行，毕竟我们只是为了虚化边缘，制造投影的效果。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">fixed4 <span class="title">Pass_SimpleBloom</span><span class="params">(float2 uv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    float4 sum = float4(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    float4 bum = float4(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    float2 glareSize = float2(<span class="number">1.0</span> / <span class="number">512</span>, <span class="number">1.0</span> / <span class="number">384</span>) * <span class="number">0.65</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> height = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">int</span> width = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = -width; i &lt; width; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = -height; j &lt; height; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            sum += tex2D(_MainTex, uv + float2(i, j) * glareSize);</span><br><span class="line">            bum += tex2D(_MainTex, uv + float2(j, i) * glareSize);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fixed4 color = PREVIOUS_PASS(uv);</span><br><span class="line">    color = (sum*sum*<span class="number">0.001</span> + bum*bum*<span class="number">0.0080</span>) * _Amount / ((<span class="number">2</span>* height +<span class="number">1</span>) *(<span class="number">2</span>* width +<span class="number">1</span>)) + color*_Power;</span><br><span class="line">    <span class="keyword">return</span> color;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="renderTexture与multipass"><a href="#renderTexture与multipass" class="headerlink" title="renderTexture与multipass"></a>renderTexture与multipass</h3><p>Bloom的操作我并没有在ppsspp模拟器中实施，主要原因是我不知道如何在ppsspp中实现真正的multi-pass shader，如果只是通过宏将pass折叠起来，由于bloom需要对周围采样，将会导致计算量指数式上涨。<br>但是这一切在unity中就很容易解决了，只需要在第一遍的pass中将bloom后的输出输出到render texture就可以被后面的shader所利用，两者加起来的时间测试下来大概只有single-pass的1/5，优化效果还是非常明显的。<br><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RenderTexture rtTemp = RenderTexture.GetTemporary(src.width, src.height);</span><br><span class="line">Graphics.Blit(src, rtTemp, _Material_1);</span><br><span class="line">Graphics.Blit(rtTemp, dst, _Material_2);</span><br><span class="line">RenderTexture.ReleaseTemporary(rtTemp);</span><br></pre></td></tr></table></figure></p> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/performance_single.png" alt title="优化之前几乎所有的时间都耗在了最后一个drawIndexed上"> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/performance.png" alt title><p>可以看出分割出两个pass之后开销一下平衡很多。另外，unity中在利用RenderTexture.GetTemporary时，内部会调用<a href="https://docs.unity3d.com/ScriptReference/RenderTexture.DiscardContents.html" target="_blank" rel="noopener">DiscardContents</a> ，因而对CPU的效率也有所提升。详情可以参考<a href="https://docs.unity3d.com/ScriptReference/RenderTexture.GetTemporary.html" target="_blank" rel="noopener">官方文档</a>。<br>增加了bloom之后的效果图。<br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/3xScaled.png" alt title><br> <img src="https://imgs-1259535704.cos.ap-guangzhou.myqcloud.com/blog/crt/result.jpg" alt title><br><br></p>]]></content>
      
      
      <categories>
          
          <category> Dev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> game </tag>
            
            <tag> graphics </tag>
            
            <tag> shader </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
